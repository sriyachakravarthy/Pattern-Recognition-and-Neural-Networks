{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy3c781_eb95",
        "outputId": "09cd50e7-3ba6-4826-a7bc-fc0b1ba23d11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3W0C3hNhb1W",
        "outputId": "e3d9a9e5-b271-4c8e-edd1-7ba44d4a67fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['POLITICS', 'WELLNESS', 'ENTERTAINMENT', 'TRAVEL', 'STYLE & BEAUTY', 'PARENTING', 'HEALTHY LIVING', 'QUEER VOICES', 'FOOD & DRINK', 'BUSINESS', 'COMEDY', 'SPORTS']\n",
            "(94684, 5000)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = pd.read_json(\"/content/drive/MyDrive/News_Category_Dataset_v3.json\", lines=True)\n",
        "\n",
        "top_categories = data['category'].value_counts().head(12).index.tolist()\n",
        "print(top_categories)\n",
        "data_top12 = data[data['category'].isin(top_categories)]\n",
        "\n",
        "X = data_top12['headline']\n",
        "y = data_top12['category']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42)\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "print(X_train_tfidf.shape)\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "wmtjbBbRPI-Y"
      },
      "outputs": [],
      "source": [
        "# def oneHotConvert(y,classes):\n",
        "#   y=y.astype(int)\n",
        "#   encoded_output = np.zeros((len(y), classes))\n",
        "#   for i in range(len(y)):\n",
        "#     # encoded_output[i][y[i]]=1\n",
        "\n",
        "#   return encoded_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "i47XTXGAWTYu"
      },
      "outputs": [],
      "source": [
        "# y_train=oneHotConvert(y_train,12)\n",
        "# y_test=oneHotConvert(y_test,12)\n",
        "# y_val=oneHotConvert(y_val,12)\n",
        "# print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "L1fJGRhzHyAa"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "    # forward pass\n",
        "    def forward_pass(self, inputs):\n",
        "        activations = inputs\n",
        "        for layer in self.layers:\n",
        "            activations = layer.forward_pass(activations, saved_weights=None)\n",
        "        return activations\n",
        "\n",
        "    # backward pass\n",
        "    def backprop(self, outputs):\n",
        "        gradients = outputs\n",
        "        for layer in reversed(self.layers):\n",
        "            gradients = layer.backprop(gradients)\n",
        "\n",
        "    # applying stochastic gradient descent (SGD)\n",
        "    def apply_sgd(self):\n",
        "        for layer in self.layers:\n",
        "            layer.apply_sgd()\n",
        "\n",
        "    # applying Adam optimizer\n",
        "    def apply_adam(self):\n",
        "        for layer in self.layers:\n",
        "            layer.apply_adam()\n",
        "\n",
        "    # changing learning rate alpha\n",
        "    def change_learning_rate(self):\n",
        "        for layer in self.layers:\n",
        "            layer.change_learning_rate()\n",
        "\n",
        "    # saving weights\n",
        "    def save_parameters(self):\n",
        "        for index, layer in enumerate(self.layers):\n",
        "            layer.save_parameters()\n",
        "\n",
        "    # predicting after loading weights\n",
        "    def predict(self, inputs):\n",
        "        activations = inputs\n",
        "        for layer in self.layers:\n",
        "            activations = layer.forward_pass(activations, saved_weights=1)\n",
        "        return activations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "Yytmn5kvH0cA"
      },
      "outputs": [],
      "source": [
        "class AdamOptimizer:\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.first_moment = None\n",
        "        self.second_moment = None\n",
        "        self.time_step = 0\n",
        "\n",
        "    def update_parameters(self, gradients):\n",
        "        if self.first_moment is None:\n",
        "            self.first_moment = np.zeros_like(gradients)\n",
        "            self.second_moment = np.zeros_like(gradients)\n",
        "\n",
        "        self.time_step += 1\n",
        "        self.first_moment = self.beta1 * self.first_moment + (1 - self.beta1) * gradients\n",
        "        self.second_moment = self.beta2 * self.second_moment + (1 - self.beta2) * (gradients ** 2)\n",
        "        m_corrected = self.first_moment / (1 - self.beta1 ** self.time_step)\n",
        "        v_corrected = self.second_moment / (1 - self.beta2 ** self.time_step)\n",
        "        return self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "SwjEdmt7H2GD"
      },
      "outputs": [],
      "source": [
        "class SoftmaxClassifier:\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def encode_one_hot(self, actual_pred):\n",
        "        # Example of one-hot encoding with numpy\n",
        "        one_hot_encoded = np.zeros(self.num_classes)\n",
        "        one_hot_encoded[actual_pred] = 1\n",
        "        return one_hot_encoded\n",
        "\n",
        "    def forward_pass(self, logits, saved_weights=None):\n",
        "        # Adjust the logits to avoid numerical instability\n",
        "        shifted_logits = logits - np.max(logits, axis=0, keepdims=True)\n",
        "\n",
        "        # Compute exponentiated values of the adjusted logits\n",
        "        exp_values = np.exp(shifted_logits)\n",
        "\n",
        "        # Normalize the exponentiated values to get softmax probabilities\n",
        "        self.softmax_probabilities = exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
        "\n",
        "        return self.softmax_probabilities\n",
        "\n",
        "    def backprop(self, target_labels):\n",
        "        # Compute one-hot encoded labels\n",
        "        one_hot_labels = self.encode_one_hot(target_labels)\n",
        "\n",
        "        # Calculate gradient of the loss function\n",
        "        gradient = (self.softmax_probabilities - one_hot_labels)\n",
        "        return gradient\n",
        "\n",
        "    def apply_sgd(self):\n",
        "        # Placeholder for applying stochastic gradient descent updates\n",
        "        pass\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        # Placeholder for updating the learning rate\n",
        "        pass\n",
        "\n",
        "    def apply_adam(self):\n",
        "        # Placeholder for applying Adam optimizer updates\n",
        "        pass\n",
        "\n",
        "    def save_parameters(self):\n",
        "        # Placeholder for saving parameters\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "oiY_iLpYH38L"
      },
      "outputs": [],
      "source": [
        "class FlattenLayer:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward_pass(self, input_data, saved_weights=None):\n",
        "        # Store the original shape of the data for use during backpropagation\n",
        "        self.original_shape = input_data.shape\n",
        "\n",
        "        # Flatten the input data into a 1D array\n",
        "        flattened_data = input_data.flatten()\n",
        "        return flattened_data\n",
        "\n",
        "    def backprop(self, gradient):\n",
        "        # Reshape the gradient to the original input shape during backpropagation\n",
        "        return gradient.reshape(self.original_shape)\n",
        "\n",
        "    def apply_sgd(self):\n",
        "        # Placeholder for applying stochastic gradient descent updates\n",
        "        pass\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        # Placeholder for updating the learning rate\n",
        "        pass\n",
        "\n",
        "    def apply_adam(self):\n",
        "        # Placeholder for applying Adam optimizer updates\n",
        "        pass\n",
        "\n",
        "    def save_parameters(self):\n",
        "        # Placeholder for saving parameters\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "uTIPkL6JH5vs"
      },
      "outputs": [],
      "source": [
        "class ReLUActivation:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward_pass(self, inputs, saved_weights=None):\n",
        "        self.inputs = inputs\n",
        "        return np.maximum(0, inputs)\n",
        "\n",
        "    def derivative(self):\n",
        "        return np.where(self.inputs > 0, 1, 0)\n",
        "\n",
        "    def backprop(self, gradient_from_next_layer):\n",
        "        return gradient_from_next_layer * self.derivative()\n",
        "\n",
        "    def apply_sgd(self):\n",
        "        pass\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        pass\n",
        "\n",
        "    def apply_adam(self):\n",
        "        pass\n",
        "\n",
        "    def save_parameters(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "ODqpVbl1H7or"
      },
      "outputs": [],
      "source": [
        "class LinearLayer:\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, learning_rate=0.01, layer_index=0, regularization=None, regularization_penalty=0):\n",
        "        self.weights = np.random.randn(input_dim, output_dim) / (input_dim * output_dim)\n",
        "        self.biases = np.zeros((output_dim,))\n",
        "        self.weight_optimizer = AdamOptimizer(learning_rate=learning_rate)\n",
        "        self.bias_optimizer = AdamOptimizer(learning_rate=learning_rate)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.layer_index = layer_index\n",
        "        self.regularization = regularization\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "\n",
        "    def forward_pass(self, input_data, saved_weights=None):\n",
        "        if saved_weights is not None:\n",
        "            saved_data = np.load(f'/content/drive/MyDrive/Colab Notebooks/Saved_Models/Linear_layer{self.layer_index}.npz')\n",
        "            self.weights = saved_data['arr1']\n",
        "            self.biases = saved_data['arr2']\n",
        "\n",
        "        self.input_data = input_data\n",
        "        self.z = np.dot(input_data, self.weights) + self.biases\n",
        "        return self.z\n",
        "\n",
        "    def backprop(self, grad_previous):\n",
        "        batch_size = self.input_data.shape[0]\n",
        "        self.grad_weights = np.dot(self.input_data.reshape(-1, 1), grad_previous.reshape(1, -1))\n",
        "        self.grad_biases = grad_previous.sum(axis=0) / batch_size\n",
        "        self.grad_input = np.dot(grad_previous, self.weights.T)\n",
        "\n",
        "        if self.regularization == 'l1':\n",
        "            grad_weights += self.regularization_penalty * np.sign(self.weights)\n",
        "            grad_biases += self.regularization_penalty * np.sign(self.biases)\n",
        "        elif self.regularization == 'l2':\n",
        "            grad_weights += 2 * self.regularization_penalty * self.weights\n",
        "            grad_biases += 2 * self.regularization_penalty * self.biases\n",
        "        elif self.regularization == 'elastic':\n",
        "            grad_weights += self.regularization_penalty * (0.5 * np.sign(self.weights) + 0.5 * self.weights)\n",
        "            grad_biases += self.regularization_penalty * (0.5 * np.sign(self.biases) + 0.5 * self.biases)\n",
        "\n",
        "        return self.grad_input\n",
        "\n",
        "    def apply_sgd(self):\n",
        "        self.weights -= self.learning_rate * self.grad_weights\n",
        "        self.biases -= self.learning_rate * self.grad_biases\n",
        "\n",
        "    def apply_adam(self):\n",
        "        self.weights -= self.weight_optimizer.update(self.grad_weights)\n",
        "        self.biases -= self.bias_optimizer.update(self.grad_biases)\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        self.learning_rate /= 5\n",
        "\n",
        "    def save_parameters(self):\n",
        "        np.savez(f'/content/drive/MyDrive/Colab Notebooks/Saved_Models/Linear_layer{self.layer_index}.npz', arr1=self.weights, arr2=self.biases)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "JIUBsEi9H9fq"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionLayer:\n",
        "\n",
        "    def __init__(self, dim, key_dim, learning_rate=0.01, layer_index=0, regularization=None, regularization_penalty=0):\n",
        "        self.key_dim = key_dim\n",
        "        self.weights_key = np.random.randn(dim, key_dim) / (dim * key_dim)\n",
        "        print(self.weights_key.shape)\n",
        "        self.weights_query = np.random.randn(dim, key_dim) / (dim * key_dim)\n",
        "        self.weights_value = np.random.randn(dim, key_dim) / (dim * key_dim)\n",
        "        self.optimizer_key = AdamOptimizer(learning_rate=learning_rate)\n",
        "        self.optimizer_query = AdamOptimizer(learning_rate=learning_rate)\n",
        "        self.optimizer_value = AdamOptimizer(learning_rate=learning_rate)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.layer_index = layer_index\n",
        "        self.regularization = regularization\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "\n",
        "    def compute_softmax(self, X):\n",
        "        shift = X - np.max(X, axis=1, keepdims=True)\n",
        "        exps = np.exp(shift)\n",
        "        output = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "        return output\n",
        "\n",
        "    def forward_pass(self, X, saved_weights=None):\n",
        "        if saved_weights is not None:\n",
        "            saved_data = np.load(f'/content/drive/MyDrive/Colab Notebooks/Saved_Models/SelfAttention_layer{self.layer_index}.npz')\n",
        "            self.weights_key = saved_data['arr1']\n",
        "            self.weights_query = saved_data['arr2']\n",
        "            self.weights_value = saved_data['arr3']\n",
        "        self.X=X\n",
        "        # print(X.shape)\n",
        "        # print(self.weights_query.shape)\n",
        "        self.Q = X @self.weights_query\n",
        "        # self.Q = np.matmul(X, self.weights_query)\n",
        "        self.K = np.matmul(X, self.weights_key)\n",
        "        self.V = np.matmul(X, self.weights_value)\n",
        "\n",
        "        scores = np.matmul(self.Q, self.K.T) / np.sqrt(self.key_dim)\n",
        "        self.attention_weights = self.compute_softmax(scores)\n",
        "        output = np.matmul(self.attention_weights, self.V)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backprop(self, gradient_previous):\n",
        "        self.grad_weights_value = np.matmul(np.matmul(self.X.T, self.attention_weights.T), gradient_previous)\n",
        "        t1 = np.multiply(self.attention_weights, np.matmul(gradient_previous, self.V.T))\n",
        "        t2 = self.Q - np.matmul(self.attention_weights, self.Q)\n",
        "        self.grad_weights_key = (1/np.sqrt(self.key_dim)) * np.matmul(np.matmul(self.X.T, t1), t2)\n",
        "\n",
        "        t3 = np.sum(np.multiply(self.attention_weights, np.matmul(gradient_previous, self.V.T)), axis=1)\n",
        "        t4 = np.multiply(t3, self.attention_weights)\n",
        "        t5 = np.multiply(self.attention_weights, np.matmul(gradient_previous, self.V.T)) - t4\n",
        "        self.grad_weights_query = (1/np.sqrt(self.key_dim)) * np.matmul(np.matmul(self.X.T, t5), self.K)\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def apply_sgd(self):\n",
        "        self.weights_key -= self.learning_rate * self.grad_weights_key\n",
        "        self.weights_query -= self.learning_rate * self.grad_weights_query\n",
        "        self.weights_value -= self.learning_rate * self.grad_weights_value\n",
        "\n",
        "    def apply_adam(self):\n",
        "        self.weights_key -= self.optimizer_key.update(self.grad_weights_key)\n",
        "        self.weights_query -= self.optimizer_query.update(self.grad_weights_query)\n",
        "        self.weights_value -= self.optimizer_value.update(self.grad_weights_value)\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        self.learning_rate /= 5\n",
        "\n",
        "    def save_parameters(self):\n",
        "        np.savez(f'/content/drive/MyDrive/Colab Notebooks/Saved_Models/SelfAttention_layer{self.layer_index}.npz', arr1=self.weights_key, arr2=self.weights_query, arr3=self.weights_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "y2Kw7qJNIxYU"
      },
      "outputs": [],
      "source": [
        "class CrossEntropyLoss:\n",
        "\n",
        "    def _init_(self):\n",
        "        pass\n",
        "    def compute(self, A, Y):\n",
        "        ce_loss = - np.log(A[Y])\n",
        "        return ce_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "0N_8dePDzcv3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "dRPApTplaErL"
      },
      "outputs": [],
      "source": [
        "# class PCA:\n",
        "#     def _init_(self, n_components=125):\n",
        "\n",
        "#         self.n_components = n_components\n",
        "#         self.mean = None\n",
        "#         self.components = None\n",
        "#         self.explained_variance_ratio = None\n",
        "\n",
        "#     def fit(self, X):\n",
        "\n",
        "#         n_samples, n_features = X.shape\n",
        "\n",
        "#         # Subtract the mean from the data\n",
        "#         self.mean = np.mean(X, axis=0)\n",
        "#         X_centered = X - self.mean\n",
        "\n",
        "#         # Calculate the covariance matrix\n",
        "#         covariance_matrix = np.cov(X_centered.T)\n",
        "\n",
        "#         # Calculate the eigenvalues and eigenvectors\n",
        "#         eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
        "\n",
        "#         eigenvectors = eigenvectors.T\n",
        "\n",
        "#         idxs = np.argsort(eigenvalues)[::-1]\n",
        "#         eigenvalues = eigenvalues[idxs]\n",
        "#         eigenvectors = eigenvectors[idxs]\n",
        "\n",
        "#         self.components = eigenvectors[:self.n_components]\n",
        "\n",
        "#         # Sort the eigenvalues and eigenvectors in descending order\n",
        "\n",
        "#         self.explained_variance_ratio = eigenvalues[idxs[:self.n_components]] / np.sum(eigenvalues)\n",
        "\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "\n",
        "#         if self.components is None:\n",
        "#             raise ValueError(\"You must fit the PCA model first.\")\n",
        "\n",
        "#         X_centered = X - self.mean\n",
        "#         X_projected = np.dot(X_centered, self.components.T)\n",
        "\n",
        "#         return X_projected\n",
        "\n",
        "#     def fit_transform(self, X):\n",
        "\n",
        "#         self.fit(X)\n",
        "#         X_projected = self.transform(X)\n",
        "\n",
        "#         return X_projected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "xKnPXvOanOoZ"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    position: int, the length of the sequence.\n",
        "    d_model: int, the dimensionality of the model's output.\n",
        "\n",
        "    Returns:\n",
        "    A numpy array shape (1, position, d_model) containing the positional encodings.\n",
        "    \"\"\"\n",
        "    # Create an array of positions (0, 1, ..., position-1) and reshape it to use broadcasting\n",
        "    angle_rads = np.arange(position)[:, np.newaxis]\n",
        "\n",
        "    # Compute the frequencies\n",
        "    angle_rates = 1 / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / d_model)\n",
        "\n",
        "    # Apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2] * angle_rates[:, 0::2])\n",
        "\n",
        "    # Apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2] * angle_rates[:, 1::2])\n",
        "\n",
        "    # Add a new axis for batch size at the beginning\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return pos_encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "bL7B-qay11iw"
      },
      "outputs": [],
      "source": [
        "def zeroMean(X):\n",
        "    X-=np.mean(X, axis=0)\n",
        "    return X\n",
        "\n",
        "def PCA(X, k):\n",
        "    X=zeroMean(X)\n",
        "    V= np.cov(X.T)\n",
        "    print(X.shape)\n",
        "    print(V.shape)\n",
        "    eig_value, eig_vec=np.linalg.eig(V)\n",
        "    sorted_eig_values=np.argsort(eig_value)[::-1]\n",
        "    vectors=eig_vec[:, sorted_eig_values[:k]]\n",
        "    W=vectors\n",
        "    print(W.shape)\n",
        "    # print(W[0])\n",
        "    Z=np.dot(X, W)\n",
        "    print(Z.shape)\n",
        "    return Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fldPn2617h5",
        "outputId": "55db6e6b-795f-4578-ec4e-d124765b6fba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(94684, 5000)\n",
            "(5000, 5000)\n",
            "(5000, 100)\n",
            "(94684, 100)\n"
          ]
        }
      ],
      "source": [
        "x_train_transform=X_train_tfidf.reshape(X_train_tfidf.shape[0], -1)\n",
        "# abc=zeroMean(x_train_transform)\n",
        "xyz=PCA(x_train_transform,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvTo08jAgh0m",
        "outputId": "c88044bf-51d1-4615-b02b-3c4bc62662b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(94684, 100)\n"
          ]
        }
      ],
      "source": [
        "print(xyz.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "EdKto6RLiuc1"
      },
      "outputs": [],
      "source": [
        "X_test_tfidf_dense = X_test_tfidf.toarray()\n",
        "X_test_tfidf = X_test_tfidf_dense[:, :, np.newaxis]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "GNd-JYcynlVZ"
      },
      "outputs": [],
      "source": [
        "encoding=positional_encoding(100,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "PS2w_gTRsWwE"
      },
      "outputs": [],
      "source": [
        "X_train_tfidf_dense = np.asarray(xyz)\n",
        "X_train_tfidf = X_train_tfidf_dense[:, :, np.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "VYmF8XDytUzu"
      },
      "outputs": [],
      "source": [
        "X_train_tfidf +=encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_82kwhxjULt",
        "outputId": "a9d1f915-2c6c-43b9-a4c4-8a879b1f2808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(94684, 100, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train_tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iADsP8bzXpk2",
        "outputId": "51f6b774-b23a-4075-9716-a5aaf9577332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 20)\n"
          ]
        }
      ],
      "source": [
        "completeNetwork = NeuralNetwork([\n",
        "    SelfAttentionLayer(dim=1, key_dim=20, learning_rate=0.2),\n",
        "    FlattenLayer(),\n",
        "    LinearLayer(input_dim=2000, output_dim=256, learning_rate=0.2),\n",
        "    ReLUActivation(),\n",
        "    LinearLayer(input_dim=256, output_dim=64, learning_rate=0.2),\n",
        "    ReLUActivation(),\n",
        "    LinearLayer(input_dim=64, output_dim=12, learning_rate=0.2),\n",
        "    SoftmaxClassifier(num_classes=12)\n",
        "])\n",
        "crossEntropyLoss = CrossEntropyLoss()\n",
        "total_loss = 0\n",
        "num_epochs = 5\n",
        "num_images = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Beq9ugjeXqS2",
        "outputId": "06d40e63-412e-4f5d-9887-8ab9a016902e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(94684, 100, 1)\n",
            "Epoch 1: Accuracy is 7.17333445988762541390%, Loss is 12.82558694215131112060\n",
            "(94684, 100, 1)\n",
            "Epoch 2: Accuracy is 7.17122217058848399063%, Loss is 14.23224168024199265403\n",
            "(94684, 100, 1)\n",
            "Epoch 3: Accuracy is 7.17122217058848399063%, Loss is 14.75449891162117133092\n",
            "(94684, 100, 1)\n",
            "Epoch 4: Accuracy is 7.17122217058848399063%, Loss is 15.09275575986468354017\n",
            "(94684, 100, 1)\n",
            "Epoch 5: Accuracy is 7.17122217058848399063%, Loss is 15.34386915552108199279\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    predicted_labels = []\n",
        "    print(X_train_tfidf.shape)\n",
        "    for index, data in enumerate(X_train_tfidf):\n",
        "        # print(\"before ,\",data.shape)\n",
        "        # if index %1000==0:\n",
        "        #   print(index)\n",
        "        probabilities = completeNetwork.forward_pass(data)\n",
        "        predicted_labels.append(np.argmax(probabilities))\n",
        "        # k=np.argmax(y_train[index])\n",
        "        total_loss += crossEntropyLoss.compute(probabilities, y_train[index])\n",
        "        completeNetwork.backprop(k)\n",
        "        completeNetwork.apply_sgd()\n",
        "    accuracy = np.mean(predicted_labels == y_train)\n",
        "    average_loss = total_loss /X_train_tfidf.shape[0]\n",
        "    print(f\"Epoch {epoch+1}: Accuracy is {accuracy:.20%}, Loss is {average_loss:.20f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
