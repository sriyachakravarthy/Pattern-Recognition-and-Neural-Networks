{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQmcReWWV-Ff"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GradientBoosting(object):\n",
        "    \"\"\"Super class of GradientBoostingClassifier and GradientBoostinRegressor.\n",
        "    Uses a collection of regression trees that trains on predicting the gradient\n",
        "    of the loss function.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_estimators: int\n",
        "        The number of classification trees that are used.\n",
        "    learning_rate: float\n",
        "        The step length that will be taken when following the negative gradient during\n",
        "        training.\n",
        "    min_samples_split: int\n",
        "        The minimum number of samples needed to make a split when building a tree.\n",
        "    min_impurity: float\n",
        "        The minimum impurity required to split the tree further.\n",
        "    max_depth: int\n",
        "        The maximum depth of a tree.\n",
        "    regression: boolean\n",
        "        True or false depending on if we're doing regression or classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators, learning_rate, min_samples_split,\n",
        "                 min_impurity, max_depth, regression):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_impurity = min_impurity\n",
        "        self.max_depth = max_depth\n",
        "        self.regression = regression\n",
        "        self.bar = progressbar.ProgressBar(widgets=bar_widgets)\n",
        "\n",
        "        # Square loss for regression\n",
        "        # Log loss for classification\n",
        "        self.loss = SquareLoss()\n",
        "        if not self.regression:\n",
        "            self.loss = CrossEntropy()\n",
        "\n",
        "        # Initialize regression trees\n",
        "        self.trees = []\n",
        "        for _ in range(n_estimators):\n",
        "            tree = RegressionTree(\n",
        "                    min_samples_split=self.min_samples_split,\n",
        "                    min_impurity=min_impurity,\n",
        "                    max_depth=self.max_depth)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_pred = np.full(np.shape(y), np.mean(y, axis=0))\n",
        "        for i in self.bar(range(self.n_estimators)):\n",
        "            gradient = self.loss.gradient(y, y_pred)\n",
        "            self.trees[i].fit(X, gradient)\n",
        "            update = self.trees[i].predict(X)\n",
        "            # Update y prediction\n",
        "            y_pred -= np.multiply(self.learning_rate, update)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.array([])\n",
        "        # Make predictions\n",
        "        for tree in self.trees:\n",
        "            update = tree.predict(X)\n",
        "            update = np.multiply(self.learning_rate, update)\n",
        "            y_pred = -update if not y_pred.any() else y_pred - update\n",
        "\n",
        "        if not self.regression:\n",
        "            # Turn into probability distribution\n",
        "            y_pred = np.exp(y_pred) / np.expand_dims(np.sum(np.exp(y_pred), axis=1), axis=1)\n",
        "            # Set label to the value that maximizes probability\n",
        "            y_pred = np.argmax(y_pred, axis=1)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "class GradientBoostingRegressor(GradientBoosting):\n",
        "    def __init__(self, n_estimators=200, learning_rate=0.5, min_samples_split=2,\n",
        "                 min_var_red=1e-7, max_depth=4, debug=False):\n",
        "        super(GradientBoostingRegressor, self).__init__(n_estimators=n_estimators,\n",
        "            learning_rate=learning_rate,\n",
        "            min_samples_split=min_samples_split,\n",
        "            min_impurity=min_var_red,\n",
        "            max_depth=max_depth,\n",
        "            regression=True)\n",
        "\n",
        "class GradientBoostingClassifier(GradientBoosting):\n",
        "    def __init__(self, n_estimators=200, learning_rate=.5, min_samples_split=2,\n",
        "                 min_info_gain=1e-7, max_depth=2, debug=False):\n",
        "        super(GradientBoostingClassifier, self).__init__(n_estimators=n_estimators,\n",
        "            learning_rate=learning_rate,\n",
        "            min_samples_split=min_samples_split,\n",
        "            min_impurity=min_info_gain,\n",
        "            max_depth=max_depth,\n",
        "            regression=False)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y = to_categorical(y)\n",
        "        super(GradientBoostingClassifier, self).fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LogisticLoss():\n",
        "    def __init__(self):\n",
        "        sigmoid = Sigmoid()\n",
        "        self.log_func = sigmoid\n",
        "        self.log_grad = sigmoid.gradient\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        p = self.log_func(y_pred)\n",
        "        return y * np.log(p) + (1 - y) * np.log(1 - p)\n",
        "\n",
        "    # gradient w.r.t y_pred\n",
        "    def gradient(self, y, y_pred):\n",
        "        p = self.log_func(y_pred)\n",
        "        return -(y - p)\n",
        "\n",
        "    # w.r.t y_pred\n",
        "    def hess(self, y, y_pred):\n",
        "        p = self.log_func(y_pred)\n",
        "        return p * (1 - p)\n",
        "\n",
        "\n",
        "class XGBoost(object):\n",
        "    \"\"\"The XGBoost classifier.\n",
        "\n",
        "    Reference: http://xgboost.readthedocs.io/en/latest/model.html\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_estimators: int\n",
        "        The number of classification trees that are used.\n",
        "    learning_rate: float\n",
        "        The step length that will be taken when following the negative gradient during\n",
        "        training.\n",
        "    min_samples_split: int\n",
        "        The minimum number of samples needed to make a split when building a tree.\n",
        "    min_impurity: float\n",
        "        The minimum impurity required to split the tree further.\n",
        "    max_depth: int\n",
        "        The maximum depth of a tree.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=200, learning_rate=0.001, min_samples_split=2,\n",
        "                 min_impurity=1e-7, max_depth=2):\n",
        "        self.n_estimators = n_estimators            # Number of trees\n",
        "        self.learning_rate = learning_rate          # Step size for weight update\n",
        "        self.min_samples_split = min_samples_split  # The minimum n of sampels to justify split\n",
        "        self.min_impurity = min_impurity              # Minimum variance reduction to continue\n",
        "        self.max_depth = max_depth                  # Maximum depth for tree\n",
        "\n",
        "        self.bar = progressbar.ProgressBar(widgets=bar_widgets)\n",
        "\n",
        "        # Log loss for classification\n",
        "        self.loss = LogisticLoss()\n",
        "\n",
        "        # Initialize regression trees\n",
        "        self.trees = []\n",
        "        for _ in range(n_estimators):\n",
        "            tree = XGBoostRegressionTree(\n",
        "                    min_samples_split=self.min_samples_split,\n",
        "                    min_impurity=min_impurity,\n",
        "                    max_depth=self.max_depth,\n",
        "                    loss=self.loss)\n",
        "\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y = to_categorical(y)\n",
        "\n",
        "        y_pred = np.zeros(np.shape(y))\n",
        "        for i in self.bar(range(self.n_estimators)):\n",
        "            tree = self.trees[i]\n",
        "            y_and_pred = np.concatenate((y, y_pred), axis=1)\n",
        "            tree.fit(X, y_and_pred)\n",
        "            update_pred = tree.predict(X)\n",
        "\n",
        "            y_pred -= np.multiply(self.learning_rate, update_pred)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = None\n",
        "        # Make predictions\n",
        "        for tree in self.trees:\n",
        "            # Estimate gradient and update prediction\n",
        "            update_pred = tree.predict(X)\n",
        "            if y_pred is None:\n",
        "                y_pred = np.zeros_like(update_pred)\n",
        "            y_pred -= np.multiply(self.learning_rate, update_pred)\n",
        "\n",
        "        # Turn into probability distribution (Softmax)\n",
        "        y_pred = np.exp(y_pred) / np.sum(np.exp(y_pred), axis=1, keepdims=True)\n",
        "        # Set label to the value that maximizes probability\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "oli_Z9NkV_dk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}