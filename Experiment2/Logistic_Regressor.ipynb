{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVymzqkPqtok",
        "outputId": "eba0d9c9-faaf-4118-90aa-8fe359e8c134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jXao9Vu4uPKD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9TFx70D3BkP",
        "outputId": "66030479-e8cf-4da5-b922-b71fdc22d20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-train-imgs.npz - 18.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17954/17954 [00:15<00:00, 1187.56KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-train-labels.npz - 0.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:00<00:00, 212.82KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-test-imgs.npz - 3.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3008/3008 [00:02<00:00, 1079.47KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-test-labels.npz - 0.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 19298.94KB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dataset files downloaded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    tqdm = lambda x, total, unit: x  # If tqdm doesn't exist, replace it with a function that does nothing\n",
        "    print('**** Could not import tqdm. Please install tqdm for download progressbars! (pip install tqdm) ****')\n",
        "\n",
        "# Python2 compatibility\n",
        "try:\n",
        "    input = raw_input\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "download_dict = {\n",
        "    '1) Kuzushiji-MNIST (10 classes, 28x28, 70k examples)': {\n",
        "        '1) MNIST data format (ubyte.gz)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz'],\n",
        "        '2) NumPy data format (.npz)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz'],\n",
        "    }\n",
        "}\n",
        "\n",
        "# Download a list of files\n",
        "def download_list(url_list):\n",
        "    for url in url_list:\n",
        "        path = url.split('/')[-1]\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(path, 'wb') as f:\n",
        "            total_length = int(r.headers.get('content-length'))\n",
        "            print('Downloading {} - {:.1f} MB'.format(path, (total_length / 1024000)))\n",
        "\n",
        "            for chunk in tqdm(r.iter_content(chunk_size=1024), total=int(total_length / 1024) + 1, unit=\"KB\"):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "    print('All dataset files downloaded!')\n",
        "\n",
        "def traverse_dict(d):\n",
        "    if isinstance(d, list):  # If we've hit a list of downloads, download that list\n",
        "        download_list(d)\n",
        "    else:\n",
        "        selected = list(d.keys())[0]  # Select the first option by default\n",
        "        traverse_dict(d[selected])     # Repeat with the next level\n",
        "\n",
        "traverse_dict(download_dict['1) Kuzushiji-MNIST (10 classes, 28x28, 70k examples)']['2) NumPy data format (.npz)'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSAEvI4Z5Xfi",
        "outputId": "f8b38079-e0ba-4543-cb27-e821c9f6d636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "X_train = np.load('kmnist-train-imgs.npz')['arr_0']\n",
        "y_train = np.load('kmnist-train-labels.npz')['arr_0']\n",
        "\n",
        "X_test = np.load('kmnist-test-imgs.npz')['arr_0']\n",
        "y_test = np.load('kmnist-test-labels.npz')['arr_0']\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(60000,28 * 28) / 255\n",
        "X_test = X_test.reshape(10000,28 * 28) / 255\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNnK65rn0ahb",
        "outputId": "d1077d61-818a-4dc5-e281-b6488746d4b4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_4v_-jJT2J0-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG_kLxZYLSt7"
      },
      "source": [
        "###ACCURACY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zFYC2lLdFjbC"
      },
      "outputs": [],
      "source": [
        "def accuracy(prediction,actual):\n",
        "  return np.mean(prediction == actual) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld9g6PvHMaTS"
      },
      "source": [
        "###CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Bv7JarwSF2cJ"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix(predicted,actual,n_classes):\n",
        "  conf_mat = np.zeros((n_classes, n_classes))\n",
        "  for i in range(len(predicted)):\n",
        "    conf_mat[int(actual[i])][int(predicted[i])] = conf_mat[int(actual[i])][int(predicted[i])] + 1\n",
        "\n",
        "  return conf_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoakOlS3MdB5"
      },
      "source": [
        "###F1 SCORE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_iOAPC9qKWVq"
      },
      "outputs": [],
      "source": [
        "def f1_score(predicted,actual,n_classes):\n",
        "  conf_mat = confusion_matrix(predicted,actual,n_classes)\n",
        "  f1_score = np.zeros(n_classes)\n",
        "  for i in range(n_classes):\n",
        "    tp = conf_mat[i][i]\n",
        "    fn = sum([conf_mat[k][i] for k in range(n_classes)]) - tp\n",
        "    fp = np.sum(conf_mat[i]) -tp\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1_score[i] = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "  return f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ptbfDEgBT5UV"
      },
      "outputs": [],
      "source": [
        "class OneVsRest:\n",
        "  def __init__(self,n_classes,n_features):\n",
        "    # [bias,W] for all classes\n",
        "    self.theta_cross = np.random.rand(n_classes, n_features + 1)\n",
        "\n",
        "\n",
        "  # Sigmoid function\n",
        "  def sigmoid(self,z):\n",
        "      return 1 / (1 + np.exp(-z))\n",
        "\n",
        "  # gradient descent for gradient descent\n",
        "  def gradient_descent_cross(self,X_bias, y, theta, learning_rate, n_iterations,tolrence):\n",
        "      m = len(y)\n",
        "      i = 0\n",
        "      grad_norm = 1\n",
        "      while grad_norm > tolrence:\n",
        "          h = self.sigmoid(np.dot(X_bias, theta))\n",
        "          gradient = np.dot(X_bias.T, (h - y)) / m\n",
        "          grad_norm = np.linalg.norm(gradient)\n",
        "          theta -= learning_rate * gradient\n",
        "          i = i+1\n",
        "          if i > n_iterations:\n",
        "            break\n",
        "      return theta\n",
        "\n",
        "\n",
        "  def fit(self,X_train,y_train):\n",
        "    # no of classes\n",
        "    classes = np.unique(y_train)\n",
        "\n",
        "    # no of features\n",
        "    n_features = X_train.shape[1]\n",
        "\n",
        "    # X_train and 1 is stacked\n",
        "    X_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
        "\n",
        "    for i,c in enumerate(classes):\n",
        "      # modified labels for binary one vs rest\n",
        "      new_y_train = np.where(c==y_train,1,0)\n",
        "\n",
        "      # apply gradient descent using cross entropy loss function\n",
        "      self.theta_cross[i] = self.gradient_descent_cross(X_bias,new_y_train,self.theta_cross[i],0.01,1000,1e-6)\n",
        "\n",
        "  def predict(self,X_test,y_test):\n",
        "    # stacking X_test with 1\n",
        "    new_X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
        "\n",
        "    # storing predictions data of all n binary classifiers for all test data points\n",
        "    predictions_cross = []\n",
        "\n",
        "    # predicting probabilities of all classes and for testing data point\n",
        "    predictions_probs_cross = np.zeros((self.theta_cross.shape[0],X_test.shape[0]))\n",
        "\n",
        "\n",
        "    for i in range(self.theta_cross.shape[0]):\n",
        "      predictions_probs_cross[i] = self.sigmoid(np.dot(new_X_test,self.theta_cross[i]))\n",
        "\n",
        "    predictions_cross = np.argmax(predictions_probs_cross, axis=0)\n",
        "\n",
        "    # Compute cross-entropy loss\n",
        "    loss_cross = 0.0\n",
        "    for i in range(X_test.shape[0]):\n",
        "        for j in range(self.theta_cross.shape[0]):\n",
        "            loss_cross += - (y_test[i] * np.log(predictions_probs_cross[j][i]) + (1 - y_test[i]) * np.log(1 - predictions_probs_cross[j][i]))\n",
        "\n",
        "    # Average the loss over all testing samples\n",
        "    loss_cross /= X_test.shape[0]\n",
        "\n",
        "    return predictions_cross,loss_cross\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJQaE8R6ESpw",
        "outputId": "e4eb10fe-3502-41e1-94ac-d4b12e1f2620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Linear Classifier using Cross Entropy: 43.580000000000005\n",
            "Linear Classifier f1 scores classwise using cross entropy\n",
            "[0.65625    0.43191489 0.37302726 0.50621891 0.46027172 0.25792812\n",
            " 0.46091205 0.41331709 0.35542169 0.39690107]\n",
            "CONFUSION MATRIX Linear Classifier using cross Entropy: \n",
            "[[651.   2.   1.  43.  84.  43.   0. 143.  15.  18.]\n",
            " [ 38. 406.  82.   7.  91.   5. 186.  41.  91.  53.]\n",
            " [  8. 103. 390.  53.  87.  12. 144. 103.  58.  42.]\n",
            " [ 22.  93.  19. 407.  71.  69.  55.  93. 114.  57.]\n",
            " [ 80.  35.  49.  20. 559.  19.  40.  94.  38.  66.]\n",
            " [ 29.  40. 273.  38.  35. 183. 189. 116.  88.   9.]\n",
            " [ 10.  31. 104.   3.  74.   1. 566. 170.  17.  24.]\n",
            " [ 21.  39.  45.   7. 159.   0.  63. 509. 119.  38.]\n",
            " [ 78.  61.  75.  27.  63.  84. 181.  39. 354.  38.]\n",
            " [ 47.  70.  53.   3. 206.   3.  32. 155.  98. 333.]]\n"
          ]
        }
      ],
      "source": [
        "train_limit = 60001\n",
        "test_limit = 10001\n",
        "#no of classes\n",
        "n_classes = np.unique(y_train).shape[0]\n",
        "n_features = X_train.shape[1]\n",
        "linear_classifier = OneVsRest(n_classes,n_features)\n",
        "linear_classifier.fit(X_train[:train_limit],y_train[:train_limit])\n",
        "predictions_cross,loss_cross = linear_classifier.predict(X_test[:test_limit],y_test[:test_limit])\n",
        "\n",
        "#Accuracy\n",
        "print(f\"Accuracy of Linear Classifier using Cross Entropy: {accuracy(predictions_cross,y_test[:test_limit])}\")\n",
        "\n",
        "\n",
        "# f1 score Euclidian\n",
        "print(\"Linear Classifier f1 scores classwise using cross entropy\")\n",
        "print(f1_score(predictions_cross,y_test[:test_limit],int(n_classes)))\n",
        "\n",
        "#confusion matrix\n",
        "print(\"CONFUSION MATRIX Linear Classifier using cross Entropy: \")\n",
        "print(confusion_matrix(predictions_cross,y_test[:test_limit],int(n_classes)))\n",
        "\n",
        "\n",
        "# #Emperical Loss for testing\n",
        "# print(f\"Emperical loss Testing using Cross Entropy = {loss_cross}\")\n",
        "\n",
        "# #Emperical Loss for training\n",
        "# print(f\"Emperical loss Training using Cross Entropy = {loss_cross}\")"
      ]
    }
  ]
}