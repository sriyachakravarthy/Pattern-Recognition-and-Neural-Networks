{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVymzqkPqtok",
        "outputId": "8e55f4d6-c182-4d73-f8d8-fe87f4563d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jXao9Vu4uPKD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9TFx70D3BkP",
        "outputId": "e1736c94-62b2-4ca8-f47d-920835426082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-train-imgs.npz - 18.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17954/17954 [00:13<00:00, 1290.40KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-train-labels.npz - 0.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:00<00:00, 206.69KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-test-imgs.npz - 3.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3008/3008 [00:03<00:00, 971.24KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-test-labels.npz - 0.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 13919.15KB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dataset files downloaded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    tqdm = lambda x, total, unit: x  # If tqdm doesn't exist, replace it with a function that does nothing\n",
        "    print('**** Could not import tqdm. Please install tqdm for download progressbars! (pip install tqdm) ****')\n",
        "\n",
        "# Python2 compatibility\n",
        "try:\n",
        "    input = raw_input\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "download_dict = {\n",
        "    '1) Kuzushiji-MNIST (10 classes, 28x28, 70k examples)': {\n",
        "        '1) MNIST data format (ubyte.gz)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz'],\n",
        "        '2) NumPy data format (.npz)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz'],\n",
        "    }\n",
        "}\n",
        "\n",
        "# Download a list of files\n",
        "def download_list(url_list):\n",
        "    for url in url_list:\n",
        "        path = url.split('/')[-1]\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(path, 'wb') as f:\n",
        "            total_length = int(r.headers.get('content-length'))\n",
        "            print('Downloading {} - {:.1f} MB'.format(path, (total_length / 1024000)))\n",
        "\n",
        "            for chunk in tqdm(r.iter_content(chunk_size=1024), total=int(total_length / 1024) + 1, unit=\"KB\"):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "    print('All dataset files downloaded!')\n",
        "\n",
        "def traverse_dict(d):\n",
        "    if isinstance(d, list):  # If we've hit a list of downloads, download that list\n",
        "        download_list(d)\n",
        "    else:\n",
        "        selected = list(d.keys())[0]  # Select the first option by default\n",
        "        traverse_dict(d[selected])     # Repeat with the next level\n",
        "\n",
        "traverse_dict(download_dict['1) Kuzushiji-MNIST (10 classes, 28x28, 70k examples)']['2) NumPy data format (.npz)'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSAEvI4Z5Xfi",
        "outputId": "4e2dd2ec-fed5-443a-d268-61dc0420af92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "X_train = np.load('kmnist-train-imgs.npz')['arr_0']\n",
        "y_train = np.load('kmnist-train-labels.npz')['arr_0']\n",
        "\n",
        "X_test = np.load('kmnist-test-imgs.npz')['arr_0']\n",
        "y_test = np.load('kmnist-test-labels.npz')['arr_0']\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1lhKbLn_Zj7"
      },
      "source": [
        "#Architecture:\n",
        "\n",
        "1. **Input Layer**:\n",
        "   - The input layer is responsible for passing the input data to the subsequent layers.\n",
        "\n",
        "\n",
        "2. **Convolutional Layer**:\n",
        "   - This layer performs convolution operations on the input data using learnable filters (kernels).\n",
        "\n",
        "3. **Pooling Layer**:\n",
        "   - The pooling layer reduces the spatial dimensions of the feature maps generated by the convolutional layer.\n",
        "\n",
        "4. **ReLU Layer (Rectified Linear Unit)**:\n",
        "   - The ReLU layer introduces non-linearity into the network by applying the ReLU activation function to the feature maps.\n",
        "\n",
        "5. **Reshaping Layer**:\n",
        "   - The reshaping layer reshapes the output of the preceding layers into a format suitable for feeding into fully connected layers.\n",
        "\n",
        "6. **Fully Connected (Linear) Layers**:\n",
        "   - These layers consist of neurons that are fully connected to all neurons in the previous layer.\n",
        "   \n",
        "7. **Softmax Layer**:\n",
        "   - It computes the probabilities of each class given the input and ensures that the sum of these probabilities is 1.\n",
        "\n",
        "8. **Loss Function (Cross Entropy)**:\n",
        "   - The cross-entropy loss function is used to measure the difference between the predicted probability distribution and the actual distribution (one-hot encoded labels).\n",
        "\n",
        "9. **Accuracy Calculation**:\n",
        "    - The accuracy module calculates the accuracy of the model predictions by comparing the predicted class labels with the true class labels.\n",
        "\n",
        "10. **Training Loop**:\n",
        "    - The training loop runs for multiple epochs, where each epoch consists of iterations over batches of training data. In each iteration, forward pass, backward pass (backpropagation), and optimization (applying SGD) are performed to update the parameters of the network.\n",
        "    - Learning rate adjustments based on performance thresholds are also implemented to improve convergence and accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Neural_Network:\n",
        "\n",
        "    def __init__(self, Network):\n",
        "        self.Network = Network\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        n = X\n",
        "        for i in self.Network:\n",
        "            n = i.forward_pass(n,saved_weights = None)\n",
        "        return n\n",
        "\n",
        "    def backprop(self, Y):\n",
        "        m = Y\n",
        "        for i in (reversed(self.Network)):\n",
        "            m = i.backprop(m)\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        for i in self.Network:\n",
        "            i.applying_sgd()\n",
        "\n",
        "    def change_alpha(self):\n",
        "        for i in self.Network:\n",
        "            i.change_alpha()\n",
        "\n",
        "    def saving_params(self):\n",
        "        saved_params = []\n",
        "        for i,layer in enumerate(self.Network):\n",
        "            saved_params.append(layer.saving_params())\n",
        "\n",
        "        return saved_params\n",
        "\n",
        "\n",
        "    def predict(self,X,saved_params):\n",
        "        n = X\n",
        "        for i,layer in enumerate(self.Network):\n",
        "            n = layer.forward_pass(n,saved_weights = saved_params[i])\n",
        "\n",
        "        return n\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jy-EOjAljNhR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ACCURACY"
      ],
      "metadata": {
        "id": "bsAOwuleuTP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g9H6H1SatxZ8"
      },
      "outputs": [],
      "source": [
        "class accuracy:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def value(self, out, Y):\n",
        "        self.out = np.argmax(out, axis=1)\n",
        "        return np.mean(self.out == Y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ePEYpbt5uRjN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pyJ0Dyeo_Kc"
      },
      "source": [
        "# SoftMax\n",
        "\n",
        "1. forward_pass(x) : a = softmax(x). returns a\n",
        "2. backward_prop(actual_y) : returns gradient = a - expansion(actual)\n",
        "3. expansion(actual_y) : returns one hot vector of actual_y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class softmax:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def expansion(self, t):\n",
        "        (a,) = t.shape\n",
        "        Y = np.zeros((a,10))\n",
        "        for i in range(0,a):\n",
        "            Y[i,t[i]] = 1\n",
        "        return Y\n",
        "\n",
        "    def forward_pass(self, z, saved_weights = None):\n",
        "        self.z =  z\n",
        "        (p,t) = self.z.shape\n",
        "        self.a = np.zeros((p,t))\n",
        "        for i in range(0,p):\n",
        "            denominator = np.sum(np.exp(self.z[i,:]))\n",
        "            for ii in range(0,t):\n",
        "                self.a[i,ii] = np.exp(self.z[i,ii])/denominator\n",
        "        # print(\"r_soft_for\")\n",
        "        return self.a\n",
        "\n",
        "    def backprop(self, Y):\n",
        "        y = self.expansion(Y)\n",
        "        self.grad = (self.a - y)\n",
        "        # print(\"r_soft_back\")\n",
        "        return self.grad\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass\n",
        "\n",
        "    def change_alpha(self):\n",
        "        pass\n",
        "\n",
        "    def saving_params(self):\n",
        "        return (None)\n"
      ],
      "metadata": {
        "id": "ZAe-iE-64EAj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7GEZj4DyuNTv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djxwm8IvnVUH"
      },
      "source": [
        "#Linear Layer\n",
        "1. x is input (prev layer output) and grad_forward is gradient from next layer.\n",
        "2. forward prop(x) --> (thetha)x + b\n",
        "3. back_prop(grad_forward) --> grdient wrt x,theta,b. Returns grad_x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear_Layer:\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, alpha = 0.01):\n",
        "        self.alpha = alpha\n",
        "        self.Theta = np.random.randn(in_dim, out_dim)\n",
        "        self.Theta = self.Theta / np.sum(self.Theta)\n",
        "        self.bias = np.random.randn(out_dim)\n",
        "        self.bias = self.bias / np.sum(self.bias)\n",
        "\n",
        "    def forward_pass(self, X, saved_weights = None):\n",
        "        if saved_weights != None:\n",
        "           self.Theta =  saved_weights[0]\n",
        "           self.bias = saved_weights[1]\n",
        "\n",
        "        self.X = X\n",
        "        self.z = np.matmul(X, self.Theta) + self.bias\n",
        "        # print(\"r_dense_for\")\n",
        "        return self.z\n",
        "\n",
        "\n",
        "    def backprop(self, grad_previous):\n",
        "        t= self.X.shape[0]\n",
        "        self.grad = np.matmul((self.X.transpose()), grad_previous)/t\n",
        "        self.grad_bias = (grad_previous.sum(axis=0))/t\n",
        "        self.grad_a = np.matmul(grad_previous, self.Theta.transpose())\n",
        "        # print(\"r_dense_back\")\n",
        "        return self.grad_a\n",
        "\n",
        "\n",
        "    def applying_sgd(self):\n",
        "            self.Theta = self.Theta - (self.alpha*self.grad)\n",
        "            self.bias = self.bias - (self.alpha*self.grad_bias)\n",
        "\n",
        "    def change_alpha(self):\n",
        "        self.alpha = self.alpha/10\n",
        "\n",
        "    def saving_params(self):\n",
        "      return (self.Theta, self.bias)"
      ],
      "metadata": {
        "id": "9u56YHip0rT0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FLATTEN"
      ],
      "metadata": {
        "id": "pFA4x5xluYu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class reshaping:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward_pass(self, data, saved_weights = None):\n",
        "        self.data_shape = data.shape\n",
        "\n",
        "        self.flatten = data.reshape(self.data_shape[0], self.data_shape[1]*self.data_shape[2]*self.data_shape[3])\n",
        "        # print(\"ret_reshape_for\")\n",
        "        print(self.flatten.shape)\n",
        "        return self.flatten\n",
        "\n",
        "    def backprop(self, data):\n",
        "        # print(\"r_reshape_back\")\n",
        "        return (data.reshape(self.data_shape[0], self.data_shape[1], self.data_shape[2], self.data_shape[3]))\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass\n",
        "\n",
        "    def change_alpha(self):\n",
        "        pass\n",
        "\n",
        "    def saving_params(self):\n",
        "        return (None)\n",
        "\n"
      ],
      "metadata": {
        "id": "V7xVcXYTyn7Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RELU"
      ],
      "metadata": {
        "id": "uQ4WjNO0ubjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class relu:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward_pass(self, z, saved_weights = None):\n",
        "        self.z = z\n",
        "        # print(\"r_relu_for\")\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def derivative(self, a):\n",
        "        return np.where(a > 0, 1, 0)\n",
        "\n",
        "    def backprop(self, grad_previous):\n",
        "        # print(\"r_relu_back\")\n",
        "        return grad_previous * self.derivative(self.z)\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass\n",
        "\n",
        "    def change_alpha(self):\n",
        "        pass\n",
        "\n",
        "    def saving_params(self):\n",
        "        return (None)"
      ],
      "metadata": {
        "id": "C6AYYJwNu11u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CROSS ENTROPY"
      ],
      "metadata": {
        "id": "GY1FXpehudTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class cross_entropy:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def expansion(self, t):\n",
        "        return np.eye(10)[t]\n",
        "\n",
        "    def loss(self, A, Y):\n",
        "        exp_Y = self.expansion(Y)\n",
        "\n",
        "        loss_matrix = -np.log(1 - A) * (exp_Y == 0) - np.log(A) * (exp_Y == 1)\n",
        "\n",
        "        max_log_loss = np.max(loss_matrix, axis=1, keepdims=True)\n",
        "        log_sum_exp = np.log(np.sum(np.exp(loss_matrix - max_log_loss), axis=1, keepdims=True)) + max_log_loss\n",
        "        average_loss = np.mean(log_sum_exp)\n",
        "\n",
        "        return average_loss"
      ],
      "metadata": {
        "id": "TPEO8GE_H_L6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class pooling:\n",
        "    def __init__(self, pool_size=(2, 2), strides=None):\n",
        "        self.pool_height, self.pool_width = pool_size\n",
        "        if strides is None:\n",
        "            self.strides = pool_size\n",
        "        else:\n",
        "            self.strides = strides\n",
        "\n",
        "    def forward_pass(self, input_data, saved_weights = None):\n",
        "        self.input_data_shape = input_data.shape\n",
        "        batch_size, input_channels, input_height, input_width = input_data.shape\n",
        "        output_height = (input_height - self.pool_height) // self.strides[0] + 1\n",
        "        output_width = (input_width - self.pool_width) // self.strides[1] + 1\n",
        "        self.output = np.zeros((batch_size, input_channels, output_height, output_width))\n",
        "\n",
        "        for b in range(batch_size):\n",
        "          for c in range(input_channels):\n",
        "            for i in range(output_height // self.strides[0]):\n",
        "                for j in range(output_width // self.strides[1]):\n",
        "                        self.output[b, c, i, j] = np.max(input_data[b, c, i*self.strides[0]:i*self.strides[0]+self.pool_height,\n",
        "                                            j*self.strides[1]:j*self.strides[1]+self.pool_width])\n",
        "        # print(\"r_pool_for\")\n",
        "        return self.output\n",
        "\n",
        "    def backprop(self, grad_previous):\n",
        "        batch_size, input_channels, output_height, output_width = grad_previous.shape\n",
        "        grad_input = np.zeros(self.input_data_shape)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "          for c in range(input_channels):\n",
        "            for i in range(output_height//self.strides[0]):\n",
        "                for j in range(output_width//self.strides[1]):\n",
        "                        patch = self.output[b, c, i*self.strides[0]:i*self.strides[0]+self.pool_height,\n",
        "                                            j*self.strides[1]:j*self.strides[1]+self.pool_width]\n",
        "                        max_index = np.unravel_index(np.argmax(patch), patch.shape)\n",
        "\n",
        "                        grad_input[b, c, i*self.strides[0]+max_index[0], j*self.strides[1]+max_index[1]] = grad_previous[b, c, i, j]\n",
        "        # print(\"r_pool_back\")\n",
        "        return grad_input\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass\n",
        "\n",
        "    def change_alpha(self):\n",
        "        pass\n",
        "\n",
        "    def saving_params(self):\n",
        "        return (None)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4xdrtnuOtA64"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-LXoU0yRezMb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CONV LAYER"
      ],
      "metadata": {
        "id": "NBGHklk_uiIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolutional_Layer:\n",
        "    def __init__(self, filter_dim = 3, stride = 1, pad = 1, alpha=0.01, num_of_filters = 1):\n",
        "        self.filter_dim = filter_dim\n",
        "        self.n_filters = num_of_filters\n",
        "        self.stride = stride\n",
        "        self.bias = np.random.randn(self.n_filters, 1)\n",
        "        self.bias = self.bias / np.sum(self.bias)\n",
        "        self.filter = np.random.randn(self.n_filters, self.filter_dim, self.filter_dim)\n",
        "        self.filter = self.filter/np.sum(self.filter, axis=0)\n",
        "        self.pad = pad\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def convolving(self, X, fil, dimen_x, dimen_y):\n",
        "        z = np.zeros((self.n_filters, dimen_x, dimen_y))\n",
        "        for k in range(self.n_filters):\n",
        "          for i in range(dimen_x):\n",
        "              for ii in range(dimen_y):\n",
        "                  temp = np.multiply(X[i : i+self.filter_dim, ii : ii+self.filter_dim], fil[k])\n",
        "                  z[k,i,ii] = temp.sum() + self.bias[k,0]\n",
        "        return z\n",
        "\n",
        "\n",
        "    def forward_pass(self, X, saved_weights = None):\n",
        "        if saved_weights != None:\n",
        "          self.filter = saved_weights[0]\n",
        "          self.bias = saved_weights[1]\n",
        "\n",
        "        self.X = np.pad(X , ((0, 0), (self.pad, self.pad), (self.pad, self.pad)),'constant', constant_values=0)\n",
        "        (d, p, t) = self.X.shape\n",
        "        dimen_x = int(((p - self.filter_dim)/self.stride) + 1)\n",
        "        dimen_y = int(((t - self.filter_dim)/self.stride) + 1)\n",
        "        self.z = np.zeros((d, self.n_filters, dimen_x, dimen_y))\n",
        "        for i in range(d):\n",
        "            self.z[i] = self.convolving(self.X[i], self.filter, dimen_x, dimen_y)\n",
        "\n",
        "        return self.z\n",
        "\n",
        "    def backprop(self, grad_z):\n",
        "        (d, f, p, t) = grad_z.shape\n",
        "\n",
        "        self.grads = np.zeros((d, p, t))\n",
        "        # for i in range(d):\n",
        "        #   for k in range(self.n_filters):\n",
        "        #     filter_1 = np.flip((np.flip(self.filter[k], axis = 0)), axis = 1)\n",
        "        #     self.grads[i] += self.convolving(np.pad(grad_z[i,k], ((1,1), (1,1)), 'constant', constant_values = 0), filter_1, p, t)\n",
        "\n",
        "        # self.grads /= self.n_filters\n",
        "        # self.grads = np.pad(self.grads, ((0,0),(1,1),(1,1)), 'constant', constant_values = 0)\n",
        "\n",
        "        self.grad_filter = np.zeros((self.n_filters, self.filter_dim, self.filter_dim))\n",
        "\n",
        "        for k in range(self.n_filters):\n",
        "          for i in range(self.filter_dim):\n",
        "              for ii in range(self.filter_dim):\n",
        "                  # print(grad_filter[k, i, ii].shape, grad_z[:,k,:,:].shape)\n",
        "                  self.grad_filter[k, i, ii] = (np.multiply(grad_z[:,k,:,:], self.X[:, i:p+i, ii:t+ii])).sum()\n",
        "        self.grad_filter = self.grad_filter /(grad_z.shape[2]*grad_z.shape[3])\n",
        "\n",
        "        self.grad_bias = np.zeros_like(self.bias)\n",
        "\n",
        "        for k in range(self.n_filters):\n",
        "          self.grad_bias[k] = (grad_z[:,k].sum()) /(grad_z.shape[2]*grad_z.shape[3])\n",
        "\n",
        "        return self.grads\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        self.filter = self.filter - (self.alpha*self.grad_filter)\n",
        "        self.bias = self.bias - (self.alpha*self.grad_bias)\n",
        "\n",
        "    def change_alpha(self):\n",
        "        self.alpha = self.alpha/10\n",
        "\n",
        "    def saving_params(self):\n",
        "        return (self.filter, self.bias)"
      ],
      "metadata": {
        "id": "_43bmwlMY9r5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP3bc0jA4JjB"
      },
      "outputs": [],
      "source": [
        "X_testing = X_train#.reshape(60000,1,28,28)\n",
        "Y_testing = y_train\n",
        "X_testing = X_testing/255\n",
        "al = 0.2\n",
        "\n",
        "complete_NN = Neural_Network([\n",
        "                                # padding(),\n",
        "                                Convolutional_Layer(alpha = al,num_of_filters = 3,pad = 1),\n",
        "                                pooling(),\n",
        "                                relu(),\n",
        "                                reshaping(),\n",
        "                                Linear_Layer(392, 100, alpha = al),\n",
        "                                relu(),\n",
        "                                Linear_Layer(100, 10, alpha = al),\n",
        "                                softmax()\n",
        "\n",
        "                                ])\n",
        "CE = cross_entropy()\n",
        "\n",
        "acc = accuracy()\n",
        "epochs = 10\n",
        "done = 0\n",
        "for i in range(epochs):\n",
        "    k = 0\n",
        "    for ii in range(6000, X_testing.shape[0] + 1, 6000):\n",
        "\n",
        "        out = complete_NN.forward_pass(X_testing[k:ii,:,:])\n",
        "        print(\"epoch:{} \\t batch: {} \\t loss: \\t {}\".format(i+1, int(ii/6000), CE.loss(out, Y_testing[k:ii])), end=\"\\t\")\n",
        "        accuracy_val = acc.value(out, Y_testing[k:ii])*100\n",
        "        print(\"accuracy: {}\".format(accuracy_val))\n",
        "\n",
        "        if ((accuracy_val>=80) and (done==0)):\n",
        "            complete_NN.change_alpha()\n",
        "            done += 1\n",
        "        if ((accuracy_val>=85) and (done==1)):\n",
        "            complete_NN.change_alpha()\n",
        "            done += 1\n",
        "\n",
        "        if ((accuracy_val>=90) and (done==2)):\n",
        "            complete_NN.change_alpha()\n",
        "            done += 1\n",
        "\n",
        "        if ((accuracy_val>=95) and (done==3)):\n",
        "            complete_NN.change_alpha()\n",
        "            done += 1\n",
        "\n",
        "        complete_NN.backprop(Y_testing[k:ii])\n",
        "        complete_NN.applying_sgd()\n",
        "        k = ii\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_1 = complete_NN.forward_pass(X_test)\n",
        "print(\"The accuracy on test set is {}\".format(acc.value(out_1, y_test)*100))"
      ],
      "metadata": {
        "id": "iADv2qFCiJOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClXQfZux4c7y"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_params = complete_NN.saving_params()\n",
        "len(saved_params)"
      ],
      "metadata": {
        "id": "uUwf7XVs_hAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Function to convert NumPy arrays to a serializable format\n",
        "def convert_array(arr):\n",
        "    if isinstance(arr, np.ndarray):\n",
        "        return arr.tolist()  # Convert NumPy array to Python list\n",
        "    else:\n",
        "        return arr\n",
        "\n",
        "# Convert tuples to a serializable format (e.g., JSON)\n",
        "serialized_data = json.dumps(saved_params, default=convert_array)\n",
        "\n",
        "# Define the file path in Google Drive\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Saved_Models/mnist_japanese_cnn.txt'\n",
        "\n",
        "# Write the serialized data to the file in Google Drive\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write(serialized_data)\n",
        "\n",
        "print(\"Data saved successfully to Google Drive.\")\n"
      ],
      "metadata": {
        "id": "nPPklQzFAJGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Function to convert lists back to NumPy arrays\n",
        "def convert_array(obj):\n",
        "    if isinstance(obj, list):\n",
        "        return np.array(obj)\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Define the file path in Google Drive\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Saved_Models/mnist_japanese_cnn.txt'\n",
        "\n",
        "# Read the serialized data from the file\n",
        "with open(file_path, 'r') as file:\n",
        "    serialized_data = file.read()\n",
        "\n",
        "# Deserialize the JSON data\n",
        "loaded_params = json.loads(serialized_data, object_hook=convert_array)\n",
        "\n",
        "print(\"Data loaded successfully from Google Drive.\")\n"
      ],
      "metadata": {
        "id": "QDwKwQ9TAxjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(loaded_params)"
      ],
      "metadata": {
        "id": "laqMW_u8B3yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out2 = complete_NN.predict(X_test,loaded_params)\n",
        "print(\"The testing loss is {}\".format(CE.loss(out2, y_test)))\n",
        "print(\"The accuracy on test set is {}\".format(acc.value(out2, y_test)*100))"
      ],
      "metadata": {
        "id": "_2joWWgGv3M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Hz-1HLcfzkl_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}