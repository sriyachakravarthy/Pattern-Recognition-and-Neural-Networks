{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o8vSHdSb1DxM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cvxopt as copt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CUzqR94jEuC0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeM838enF31d",
        "outputId": "c654f971-c7be-43e6-90eb-c8f1a519f7a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(70000, 26)\n"
          ]
        }
      ],
      "source": [
        "data = np.loadtxt('/content/drive/MyDrive/PRNN/Assignment_2/multi_class/multi_class_classification_data_group_5_train.txt', delimiter='\\t',skiprows=1)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_VMOLsrIIzo"
      },
      "source": [
        "Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ8Ng6G5F39t",
        "outputId": "80d19344-b8b4-4b16-d5e9-b0265ce898cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 700\n",
            "Test set size: 69300\n"
          ]
        }
      ],
      "source": [
        "train_ratio,test_ratio = 0.01,0.99\n",
        "\n",
        "np.random.shuffle(data)\n",
        "\n",
        "num_samples = len(data)\n",
        "num_train,num_test = int(train_ratio * num_samples),int(test_ratio * num_samples)\n",
        "#split data\n",
        "train_data,test_data = data[:num_train],data[num_train:]\n",
        "\n",
        "print(\"Training set size:\", len(train_data))\n",
        "print(\"Test set size:\", len(test_data))\n",
        "\n",
        "X_train = train_data[:, :-1]  # Features\n",
        "y_train = train_data[:, -1]   # Labels\n",
        "X_test = test_data[:, :-1]  # Features\n",
        "y_test = test_data[:, -1]   # Labels\n",
        "float_array = np.array(y_test)\n",
        "y_test = float_array.astype(int)\n",
        "num_classes = 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBbQKygwIxxi"
      },
      "source": [
        "Define Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Euw8V4lQF4ET"
      },
      "outputs": [],
      "source": [
        "def linear_kernel(X1, X2):\n",
        "    return np.dot(X1, X2.T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAb1YJoUz97m"
      },
      "outputs": [],
      "source": [
        "def polynomial_kernel(X1, X2, degree=3):\n",
        "    return (np.dot(X1, X2.T) + 1) ** degree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPgJXrVZz-a8"
      },
      "outputs": [],
      "source": [
        "def rbf_kernel(X1, X2, gamma=1.0):\n",
        "    n1 = np.shape(X1)[0]\n",
        "    n2 = np.shape(X2)[0]\n",
        "    K = np.zeros((n1, n2))\n",
        "    for i in range(n1):\n",
        "        for j in range(n2):\n",
        "            K[i,j] = np.exp(-gamma * np.linalg.norm(X1[i] - X2[j])**2)\n",
        "    return K\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRMwObsnI38q"
      },
      "source": [
        "Define Optimization function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CEDWipOeF4KO"
      },
      "outputs": [],
      "source": [
        "def optimize_dual(X, y, kernel, C):\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "    # Compute the Gram matrix\n",
        "    K = kernel(X, X)\n",
        "\n",
        "    # Define the quadratic and linear terms of the QP problem\n",
        "    P = copt.matrix(np.outer(y, y) * K)\n",
        "    q = copt.matrix(-np.ones(n_samples))\n",
        "    G = copt.matrix(np.vstack((-np.eye(n_samples), np.eye(n_samples))))\n",
        "    h = copt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)))\n",
        "    A = copt.matrix(y.astype(float), (1, n_samples))\n",
        "    b = copt.matrix(0.0)\n",
        "    # Solve the QP problem\n",
        "    # cvxopt.solvers.options['show_progress']=False\n",
        "    sol = copt.solvers.qp(P, q, G, h, A, b)\n",
        "\n",
        "    # Extract lagrange multipliers\n",
        "    alpha = np.array(sol['x'])\n",
        "    return alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUJbnPlBJK38"
      },
      "source": [
        "Training Function for OneVsRest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ndZDRDXSJIkB"
      },
      "outputs": [],
      "source": [
        "def train_svm_one_vs_rest(X_train, y_train, kernel, C):\n",
        "    n_samples, n_features = X_train.shape\n",
        "    n_classes = len(np.unique(y_train))\n",
        "    models = []\n",
        "\n",
        "    '''for i in range(n_classes):\n",
        "        # Convert the problem to binary classification: class i vs rest\n",
        "        binary_y_train = np.where(y_train == i, 1, -1)\n",
        "        alpha = optimize_dual(X_train, binary_y_train, kernel, C)\n",
        "\n",
        "        # Compute support vectors\n",
        "        sv_idx = alpha > 0\n",
        "        sv_idx = sv_idx.flatten()\n",
        "        support_vectors = X_train[sv_idx]\n",
        "        support_vector_labels = binary_y_train[sv_idx]\n",
        "        alpha_sv = alpha[sv_idx]\n",
        "\n",
        "        # Compute bias term\n",
        "        kernel_matrix = kernel(support_vectors, support_vectors)\n",
        "        alpha_sv = alpha_sv.reshape(-1,)\n",
        "        product = (support_vector_labels * alpha_sv)\n",
        "        decision_values = np.dot(kernel_matrix, product)\n",
        "        bias = np.mean(support_vector_labels - decision_values)\n",
        "\n",
        "        # Store the trained model\n",
        "        models.append({'class': i, 'support_vectors': support_vectors, 'support_vector_labels': support_vector_labels,\n",
        "                       'alpha_sv': alpha_sv, 'bias': bias})\n",
        "        return models'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEPV75aJJTAG"
      },
      "source": [
        "Predict Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lkVGGhBtJI03"
      },
      "outputs": [],
      "source": [
        "def predict_svm_one_vs_rest(X_test, models, kernel):\n",
        "    n_samples_test = X_test.shape[0]\n",
        "    n_classes = len(models)\n",
        "    decision_functions = np.zeros((n_samples_test, n_classes))\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        decision_function = np.dot(kernel(X_test, model['support_vectors']), (model['support_vector_labels'] * model['alpha_sv'])) + model['bias']\n",
        "        decision_functions[:, i] = decision_function\n",
        "\n",
        "    predicted_labels = np.argmax(decision_functions, axis=1)\n",
        "    return predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6O_j2Of2JVat",
        "outputId": "d25d87cc-fac2-4634-9e84-7cda6ccad07c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -1.5164e-03 -7.0143e+02  3e+03  1e+00  2e-14\n",
            " 1:  1.0738e-01 -1.8371e+02  2e+02  2e-02  2e-14\n",
            " 2:  4.9218e-02 -1.8945e+01  2e+01  2e-03  2e-14\n",
            " 3:  3.8288e-02 -6.0775e+00  7e+00  5e-04  2e-14\n",
            " 4:  3.3662e-02 -2.0320e+00  2e+00  1e-04  1e-14\n",
            " 5: -2.5021e-02 -1.0272e-01  8e-02  1e-06  8e-15\n",
            " 6: -3.7750e-02 -5.9593e-02  2e-02  3e-07  5e-15\n",
            " 7: -4.1524e-02 -4.8468e-02  7e-03  8e-08  5e-15\n",
            " 8: -4.3120e-02 -4.4761e-02  2e-03  2e-16  5e-15\n",
            " 9: -4.3480e-02 -4.3643e-02  2e-04  2e-16  4e-15\n",
            "10: -4.3525e-02 -4.3531e-02  5e-06  2e-16  5e-15\n",
            "11: -4.3527e-02 -4.3527e-02  2e-07  2e-16  5e-15\n",
            "12: -4.3527e-02 -4.3527e-02  7e-09  2e-16  5e-15\n",
            "Optimal solution found.\n",
            "Accuracy: 0.09968253968253968\n"
          ]
        }
      ],
      "source": [
        "models = train_svm_one_vs_rest(X_train, y_train, polynomial_kernel, C=1.0)\n",
        "\n",
        "# Predict using OvR SVM models\n",
        "y_pred = predict_svm_one_vs_rest(X_test, models, polynomial_kernel)\n",
        "\n",
        "# Evaluate accuracy or other metrics\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kdFoYYS8KxM1"
      },
      "outputs": [],
      "source": [
        "#let us define optimization function without regularization Parameter C\n",
        "'''def optimize_dual_C(X, y, kernel):\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "    # Compute the Gram matrix\n",
        "    K = kernel(X, X)\n",
        "\n",
        "    # Define the quadratic and linear terms of the QP problem\n",
        "    P = copt.matrix(np.outer(y, y) * K)\n",
        "    q = copt.matrix(-np.ones(n_samples))\n",
        "    G = copt.matrix(-np.eye(n_samples))  # Removed positive identity matrix\n",
        "    h = copt.matrix(np.zeros(n_samples))  # Removed C values\n",
        "    A = copt.matrix(y.astype(float), (1, n_samples))\n",
        "    b = copt.matrix(0.0)\n",
        "    # Solve the QP problem\n",
        "    solution = copt.solvers.qp(P, q, G, h, A, b)\n",
        "\n",
        "    # Extract lagrange multipliers\n",
        "    alpha = np.array(solution['x'])\n",
        "    return alpha'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eUgoVfD9Mia6"
      },
      "outputs": [],
      "source": [
        "def train_svm_one_vs_rest_C(X_train, y_train, kernel):\n",
        "    n_samples, n_features = X_train.shape\n",
        "    n_classes = len(np.unique(y_train))\n",
        "    models = []\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        # Convert the problem to binary classification: class i vs rest\n",
        "        binary_y_train = np.where(y_train == i, 1, -1)\n",
        "        alpha = optimize_dual_C(X_train, binary_y_train, kernel)\n",
        "\n",
        "        # Compute support vectors\n",
        "        sv_idx = alpha > 0\n",
        "        sv_idx = sv_idx.flatten()\n",
        "        support_vectors = X_train[sv_idx]\n",
        "        support_vector_labels = binary_y_train[sv_idx]\n",
        "        alpha_sv = alpha[sv_idx]\n",
        "\n",
        "        # Compute bias term\n",
        "        kernel_matrix = kernel(support_vectors, support_vectors)\n",
        "        alpha_sv = alpha_sv.reshape(-1,)\n",
        "        product = (support_vector_labels * alpha_sv)\n",
        "        decision_values = np.dot(kernel_matrix, product)\n",
        "        bias = np.mean(support_vector_labels - decision_values)\n",
        "\n",
        "        # Store the trained model\n",
        "        models.append({'class': i, 'support_vectors': support_vectors, 'support_vector_labels': support_vector_labels,\n",
        "                       'alpha_sv': alpha_sv, 'bias': bias})\n",
        "        return models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xBh_qpLrMwd5"
      },
      "outputs": [],
      "source": [
        "#Predict Function\n",
        "def predict_svm_one_vs_rest_C(X_test, models, kernel):\n",
        "    n_samples_test = X_test.shape[0]\n",
        "    n_classes = len(models)\n",
        "    decision_functions = np.zeros((n_samples_test, n_classes))\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        decision_function = np.dot(kernel(X_test, model['support_vectors']), (model['support_vector_labels'] * model['alpha_sv'])) + model['bias']\n",
        "        decision_functions[:, i] = decision_function\n",
        "\n",
        "    predicted_labels = np.argmax(decision_functions, axis=1)\n",
        "    return predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoQ_JhDPNCgI",
        "outputId": "0b697bc7-36ac-432f-c4cc-b368fc83ffe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -5.6821e-02 -1.7153e-01  7e+02  3e+01  1e+00\n",
            " 1: -1.3367e-03 -2.2721e-01  8e+00  3e-01  1e-02\n",
            " 2: -2.6667e-02 -1.6661e-01  9e-01  3e-02  1e-03\n",
            " 3: -4.0000e-02 -1.0883e-01  2e-01  6e-03  2e-04\n",
            " 4: -3.7296e-02 -8.1088e-02  1e-01  2e-03  7e-05\n",
            " 5: -3.9587e-02 -5.2468e-02  1e-02  6e-05  2e-06\n",
            " 6: -4.2130e-02 -4.5859e-02  4e-03  1e-05  5e-07\n",
            " 7: -4.3181e-02 -4.3886e-02  7e-04  5e-18  5e-15\n",
            " 8: -4.3490e-02 -4.3559e-02  7e-05  7e-18  4e-15\n",
            " 9: -4.3526e-02 -4.3528e-02  3e-06  6e-18  5e-15\n",
            "10: -4.3527e-02 -4.3527e-02  1e-07  8e-18  5e-15\n",
            "11: -4.3527e-02 -4.3527e-02  5e-09  1e-18  5e-15\n",
            "Optimal solution found.\n",
            "Accuracy: 0.09968253968253968\n"
          ]
        }
      ],
      "source": [
        "models = train_svm_one_vs_rest_C(X_train, y_train, polynomial_kernel)\n",
        "\n",
        "# Predict using OvR SVM models\n",
        "y_pred = predict_svm_one_vs_rest_C(X_test, models, polynomial_kernel)\n",
        "\n",
        "# Evaluate accuracy or other metrics\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
